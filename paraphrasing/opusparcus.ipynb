{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_checkpoint = 'cointegrated/rubert-base-cased-nli-twoway'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "def predict_pair(text1, text2):\n",
    "    with torch.inference_mode():\n",
    "        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n",
    "        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n",
    "        return {v: proba[k] for k, v in model.config.id2label.items()}['entailment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('opusparcus_v1/ru/dev/ru-dev.txt', sep='\\t', names=['id', 'text1', 'text2', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('opusparcus_v1/ru/test/ru-test.txt', sep='\\t', names=['id', 'text1', 'text2', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_dev, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "scores = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows()):\n",
    "    scores.append((predict_pair(row['text1'], row['text2']),\n",
    "                   predict_pair(row['text2'], row['text1'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ent1'] = [s[0] for s in scores]\n",
    "df['ent2'] = [s[1] for s in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(df['score'], df['ent1'] * df['ent2'], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ent1']*df['ent2']) > 0.6].score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('opusparcus_entail_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "reader = pd.read_csv('opusparcus_v1/ru/train/ru-train.txt', sep='\\t',\n",
    "                 names=['id', 'text1', 'text2', 's1', 's2', 's3', 's4'],\n",
    "                 chunksize=512)\n",
    "\n",
    "with open('opusparcus_entail_train.csv', 'w', encoding='utf8') as f:\n",
    "    for df in tqdm.tqdm(reader, total=22118837//512):\n",
    "        for _, row in df.iterrows():\n",
    "            if row['s1'] < 7:\n",
    "                raise\n",
    "            ent1 = predict_pair(row['text1'], row['text2'])\n",
    "            ent2 = predict_pair(row['text2'], row['text1'])\n",
    "            f.write(row['id']+'\\t'+row['text1'] +'\\t'+row['text2']+'\\t'+str(float(ent1))+'\\t'+str(float(ent2))+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "further-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/opusparcus_entailment.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id', 'text1', 'text2', 'ent1', 'ent2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rental-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.ent1 > 0.7) & (df.ent2 > 0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "synthetic-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.text1.str.lower().str.contains('чёрт|черт|бля|хрен|целк') | df.text2.str.lower().str.contains('чёрт|черт|бля|хрен|целк'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "micro-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.text1.str.len() + df.text2.str.len()) > 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cellular-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.text1.str.split().apply(len) > 3) & (df.text2.str.split().apply(len) > 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gentle-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.text1.apply(lambda x: '...' in x)) | (df.text2.apply(lambda x: '...' in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "desperate-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vity(row):\n",
    "    if 'вы' in row['text1'].lower() and 'ты' in row['text2'].lower():\n",
    "        return True\n",
    "    elif 'вы' in row['text2'].lower() and 'ты' in row['text1'].lower():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "overhead-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.apply(vity, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sealed-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabet_detector import AlphabetDetector\n",
    "ad = AlphabetDetector()\n",
    "\n",
    "\n",
    "df = df[~(df.text1.apply(lambda x: ad.is_latin(x)))]\n",
    "df = df[~(df.text2.apply(lambda x: ad.is_latin(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alpha-george",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687017"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "little-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wired-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['leven'] = df.apply(lambda x: similar(x['text1'], x['text2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bronze-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['leven'] < 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "falling-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652591"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "distinguished-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "sw = ('ну', 'хуй', 'хуя', 'ээ', 'эээ', 'мэм',\n",
    "      'ох', 'ах', 'вау', 'бог', 'боже', 'эй', '...', '—')\n",
    "def stopwords(s):\n",
    "    for tok in tokenize(s):\n",
    "        if tok.text.lower() in sw:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "standard-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(s):\n",
    "    toks = [_.text for _ in tokenize(s)]\n",
    "    if len(toks) / len(set(toks)) > 2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "going-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.text1.apply(stopwords)]\n",
    "df = df[~df.text2.apply(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "royal-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.text1.apply(repeat)]\n",
    "df = df[~df.text2.apply(repeat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "specified-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.text1.apply(lambda x: 'хер' in x)]\n",
    "df = df[~df.text2.apply(lambda x: 'хер' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "valuable-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.id.isin(['ru-N2362921', 'ru-N7967249'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "running-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.text1.str.contains('l')]\n",
    "df = df[~df.text2.str.contains('l')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "emerging-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599379"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "harmful-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "from collections import Counter\n",
    "from razdel import tokenize\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "def match_ents(s1, s2):\n",
    "    doc1, doc2 = Doc(s1), Doc(s2)\n",
    "    \n",
    "    doc1.segment(segmenter)\n",
    "    doc2.segment(segmenter)\n",
    "    \n",
    "    doc1.tag_ner(ner_tagger)\n",
    "    doc2.tag_ner(ner_tagger)\n",
    "    \n",
    "    if Counter([s.type for s in doc1.spans]) != Counter([s.type for s in doc2.spans]):\n",
    "        return False\n",
    "    \n",
    "    numbers1 = set()\n",
    "    numbers2 = set()\n",
    "    \n",
    "    for tok in doc1.tokens:\n",
    "        if has_numbers(tok.text):\n",
    "            numbers1.add(tok.text)\n",
    "    \n",
    "    for tok in doc2.tokens:\n",
    "        if has_numbers(tok.text):\n",
    "            numbers2.add(tok.text)\n",
    "    \n",
    "    if numbers1 != numbers2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "    \n",
    "\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def match_abbr(s1, s2):\n",
    "    abbrs1 = set()\n",
    "    abbrs2 = set()\n",
    "    \n",
    "    for tok in tokenize(s1):\n",
    "        if tok.text.isupper() and len(tok.text) > 1:\n",
    "            abbrs1.add(tok.text)\n",
    "    for tok in tokenize(s2):\n",
    "        if tok.text.isupper() and len(tok.text) > 1:\n",
    "            abbrs2.add(tok.text)\n",
    "    if abbrs1 == abbrs2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_latin(s1, s2):\n",
    "    lat1 = set()\n",
    "    lat2 = set()\n",
    "    \n",
    "    for tok in tokenize(s1):\n",
    "        if ad.islatin(tok.text):\n",
    "            lat1.add(tok.text)\n",
    "    for tok in tokenize(s2):\n",
    "        if ad.islatin(tok.text):\n",
    "            lat2.add(tok.text)\n",
    "\n",
    "    if lat1 == lat2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "appreciated-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda x: match_abbr(x['text1'], x['text2']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "objective-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598240/598240 [20:20<00:00, 490.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ner_data = []\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    if match_ents(row['text1'], row['text2']):\n",
    "        ner_data.append(row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bound-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.id.isin(ner_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "temporal-apple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572276"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "exempt-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same(s1, s2):\n",
    "    toks1 = set([_.text for _ in tokenize(s1) if _.text.isalpha()])\n",
    "    toks2 = set([_.text for _ in tokenize(s2) if _.text.isalpha()])\n",
    "    \n",
    "    if toks1 == toks2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "paperback-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.apply(lambda x: same(x['text1'], x['text2']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pending-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/clean/opusparcus_clean.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "early-challenge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563332"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lonely-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "italian-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_gender(s1, s2):\n",
    "    try:\n",
    "        g = set()\n",
    "        d1 = Doc(s1)\n",
    "        d1.segment(segmenter)\n",
    "        d1.parse_syntax(syntax_parser)\n",
    "        d1.tag_morph(morph_tagger)\n",
    "\n",
    "        d2 = Doc(s2)\n",
    "        d2.segment(segmenter)\n",
    "        d2.parse_syntax(syntax_parser)\n",
    "        d2.tag_morph(morph_tagger)\n",
    "\n",
    "        root_g1 = g.add([_.feats['Gender'] for _ in d1.tokens if _.rel =='root'][0])\n",
    "        root_g2 = g.add([_.feats['Gender'] for _ in d2.tokens if _.rel =='root'][0])\n",
    "    #     print(g)\n",
    "        if g == {'Fem', 'Masc'}:\n",
    "            return False\n",
    "    except:\n",
    "        pass\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "atmospheric-residence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_gender('Уверена , что с ним всё в порядке .', 'Я уверен , он в порядке .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "direct-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563332/563332 [37:21<00:00, 251.36it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_data = []\n",
    "\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    if root_gender(row['text1'], row['text2']):\n",
    "        gen_data.append(row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "freelance-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.id.isin(gen_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "colonial-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536985"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "chief-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536985/536985 [00:35<00:00, 15084.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    c.update(Counter(row['text1']))\n",
    "    c.update(Counter(row['text2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "empirical-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/clean/opusparcus_clean.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мужской vs женский род (по глаголу центральному root) --> можно попробовать повосстанавливать\n",
    "спец символы\n",
    "# 1 слово vs дофига слов\n",
    "буква ё\n",
    "# начало с ---\n",
    "# многоточие?\n",
    "# Маты убрать и междометия\n",
    "# Фразы на иностранном языке \n",
    "# NER filtering\n",
    "# числа? \n",
    "# повторы (нет нет нет)\n",
    "думаешь vs думаете (вы ты в root)\n",
    "# ru-N2362921, ru-N7967249 wtf\n",
    "# мб удалить то что в скобках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-falls",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
