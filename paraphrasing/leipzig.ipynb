{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "insured-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_checkpoint = 'cointegrated/rubert-base-cased-nli-twoway'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "def predict_pair(text1, text2):\n",
    "    with torch.inference_mode():\n",
    "        out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\n",
    "        proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n",
    "        return {v: proba[k] for k, v in model.config.id2label.items()}['entailment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "optical-cambodia",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/paranmt_ru_leipzig.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bfda40b0d921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/raw/paranmt_ru_leipzig.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/paranmt_ru_leipzig.tsv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/raw/paranmt_ru_leipzig.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hairy-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>original</th>\n",
       "      <th>en</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276827</td>\n",
       "      <td>Жильцам многоквартирных домов часто приходится...</td>\n",
       "      <td>Residents of apartment buildings often have to...</td>\n",
       "      <td>Жителям многоквартирного дома часто приходится...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>849426</td>\n",
       "      <td>Так или иначе, желания наши все равно дадут о ...</td>\n",
       "      <td>In any case, our desires will still be felt.</td>\n",
       "      <td>В любом случае, наши желания все равно будут о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504500</td>\n",
       "      <td>Негативные события станут позитивными тогда, к...</td>\n",
       "      <td>Negative events become positive when a person ...</td>\n",
       "      <td>События становятся позитивными, если человек м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>601055</td>\n",
       "      <td>Они уже стали чем-то вроде второй кожи в моем ...</td>\n",
       "      <td>They have already become something of a second...</td>\n",
       "      <td>Они уже превратились для меня в нечто вроде \"в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>980222</td>\n",
       "      <td>Юноши и девушки нового поколения имеют те же п...</td>\n",
       "      <td>Young men and women of the new generation have...</td>\n",
       "      <td>У юношей и девушек нового поколения одни и те ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx                                           original  \\\n",
       "0  276827  Жильцам многоквартирных домов часто приходится...   \n",
       "1  849426  Так или иначе, желания наши все равно дадут о ...   \n",
       "2  504500  Негативные события станут позитивными тогда, к...   \n",
       "3  601055  Они уже стали чем-то вроде второй кожи в моем ...   \n",
       "4  980222  Юноши и девушки нового поколения имеют те же п...   \n",
       "\n",
       "                                                  en  \\\n",
       "0  Residents of apartment buildings often have to...   \n",
       "1       In any case, our desires will still be felt.   \n",
       "2  Negative events become positive when a person ...   \n",
       "3  They have already become something of a second...   \n",
       "4  Young men and women of the new generation have...   \n",
       "\n",
       "                                                  ru  \n",
       "0  Жителям многоквартирного дома часто приходится...  \n",
       "1  В любом случае, наши желания все равно будут о...  \n",
       "2  События становятся позитивными, если человек м...  \n",
       "3  Они уже превратились для меня в нечто вроде \"в...  \n",
       "4  У юношей и девушек нового поколения одни и те ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "strange-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [5:41:12<00:00, 48.85it/s] \n"
     ]
    }
   ],
   "source": [
    "# import tqdm\n",
    "\n",
    "# with open('leipzig_entailment.tsv', 'w', encoding='utf8') as f:\n",
    "#     for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "#         ent1 = str(float(predict_pair(row['original'], row['ru'])))\n",
    "#         ent2 = str(float(predict_pair(row['ru'], row['original'])))\n",
    "#         f.write('\\t'.join([str(row['idx']), row['original'], row['ru'], ent1, ent2]) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "flying-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/leipzig_entailment.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "aerial-airplane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "specific-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.ent1 > 0.6) & (df.ent2 > 0.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "voluntary-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabet_detector import AlphabetDetector\n",
    "ad = AlphabetDetector()\n",
    "\n",
    "\n",
    "df = df[~(df.text1.apply(lambda x: ad.is_latin(x)))]\n",
    "df = df[~(df.text2.apply(lambda x: ad.is_latin(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "sound-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    texts = (row['text1'], row['text2'])\n",
    "    for t in texts:\n",
    "        for char in t:\n",
    "            c[char] += 1\n",
    "\n",
    "filter_symbols = set([row[0] for row in c.most_common() if row[1] <= 459])\n",
    "filter_symbols = set([ch for ch in filter_symbols if ch.strip() not in punctuation])\n",
    "def filter_bad_char(s):\n",
    "    s = set(s)\n",
    "    for char in filter_symbols:\n",
    "        if char in s:\n",
    "            return True\n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "boxed-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.text1.apply(filter_bad_char)]\n",
    "df = df[~df.text2.apply(filter_bad_char)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "alive-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "from collections import Counter\n",
    "from razdel import tokenize\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "def match_ents(s1, s2):\n",
    "    doc1, doc2 = Doc(s1), Doc(s2)\n",
    "    \n",
    "    doc1.segment(segmenter)\n",
    "    doc2.segment(segmenter)\n",
    "    \n",
    "    doc1.tag_ner(ner_tagger)\n",
    "    doc2.tag_ner(ner_tagger)\n",
    "    \n",
    "    if Counter([s.type for s in doc1.spans]) != Counter([s.type for s in doc2.spans]):\n",
    "        return False\n",
    "    \n",
    "    numbers1 = set()\n",
    "    numbers2 = set()\n",
    "    \n",
    "    for tok in doc1.tokens:\n",
    "        if has_numbers(tok.text):\n",
    "            numbers1.add(tok.text)\n",
    "    \n",
    "    for tok in doc2.tokens:\n",
    "        if has_numbers(tok.text):\n",
    "            numbers2.add(tok.text)\n",
    "    \n",
    "    if numbers1 != numbers2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "    \n",
    "\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def match_abbr(s1, s2):\n",
    "    abbrs1 = set()\n",
    "    abbrs2 = set()\n",
    "    \n",
    "    for tok in tokenize(s1):\n",
    "        if tok.text.isupper() and len(tok.text) > 1:\n",
    "            abbrs1.add(tok.text)\n",
    "    for tok in tokenize(s2):\n",
    "        if tok.text.isupper() and len(tok.text) > 1:\n",
    "            abbrs2.add(tok.text)\n",
    "    if abbrs1 == abbrs2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_latin(s1, s2):\n",
    "    lat1 = set()\n",
    "    lat2 = set()\n",
    "    \n",
    "    for tok in tokenize(s1):\n",
    "        if ad.islatin(tok.text):\n",
    "            lat1.add(tok.text)\n",
    "    for tok in tokenize(s2):\n",
    "        if ad.islatin(tok.text):\n",
    "            lat2.add(tok.text)\n",
    "\n",
    "    if lat1 == lat2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "physical-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyrillic\n",
    "# abbr\n",
    "# numbers match\n",
    "# ents match\n",
    "# ты вы\n",
    "# кавычки заменить на ёлочки все\n",
    "# uniq ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "japanese-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda x: match_abbr(x['text1'], x['text2']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "active-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523230/523230 [29:21<00:00, 296.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ner_data = []\n",
    "for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    if match_ents(row['text1'], row['text2']):\n",
    "        ner_data.append(row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "united-harbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text1', 'text2', 'ent1', 'ent2'], dtype='object')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "announced-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.id.isin(ner_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "different-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1, doc2 = Doc(s1), Doc(s2)\n",
    "\n",
    "doc1.segment(segmenter)\n",
    "doc2.segment(segmenter)\n",
    "\n",
    "doc1.tag_ner(ner_tagger)\n",
    "doc2.tag_ner(ner_tagger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "smaller-bones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "political-hours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(stop=10, type='LOC', text='Где-нибудь', tokens=[...])]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "champion-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2 = k['text1'], k['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "minimal-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В то же время встречаются родители, которые полностью одобряют увлечение детей, понимая, что просмотр аниме ничуть не опаснее комиксов или фильмов.\n",
      "Вместе с тем, есть родители, полностью одобряющие увлечение своих детей, зная, что смотреть мультфильмы не более опасно, чем комедии или фильмы.\n"
     ]
    }
   ],
   "source": [
    "k = df.sample(1).iloc[0]\n",
    "print(k['text1'])\n",
    "print(k['text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/clean/leipzig_clean.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
